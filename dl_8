dl 10 experiment

import numpy as np
import pandas as pd
import tensorflow as tf
import re
import matplotlib.pyplot as plt
import seaborn as sns
import pickle

from tensorflow.keras.layers import SimpleRNN, LSTM, GRU, Bidirectional, Dense, Embedding
from tensorflow.keras.datasets import imdb
from tensorflow.keras.models import Sequential
from tensorflow.keras.preprocessing import sequence

# Load IMDB dataset with top 5000 words
vocab_size = 5000
(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size)

# Print a sample review (as word indices)
print("Sample review (word indices):")
print(x_train[0])

# Get word index mapping
word_idx = imdb.get_word_index()
# Reverse the word index to map indices to words
word_idx = {i: word for word, i in word_idx.items()}

# Print the same sample review (as words)
print("\nSample review (words):")
print([word_idx.get(i, '?') for i in x_train[0]])

# Get the minimum and maximum length of reviews
print("\nMax length of a review:", len(max((x_train+x_test), key=len)))
print("Min length of a review:", len(min((x_train+x_test), key=len)))

# Pad sequences to a fixed length of 400 words
max_words = 400
x_train = sequence.pad_sequences(x_train, maxlen=max_words)
x_test = sequence.pad_sequences(x_test, maxlen=max_words)

# Split training data into training and validation sets
x_valid, y_valid = x_train[:64], y_train[:64]
x_train, y_train = x_train[64:], y_train[64:]

# Set embedding length
embd_len = 32

# Create RNN model
RNN_model = Sequential(name="Simple_RNN")
RNN_model.add(Embedding(vocab_size, 
                       embd_len, 
                       input_length=max_words))
# Add SimpleRNN layer
RNN_model.add(SimpleRNN(128, 
                       activation='tanh', 
                       return_sequences=False))
# Add output layer
RNN_model.add(Dense(1, activation='sigmoid'))

# Print model summary
print("\nModel Summary:")
print(RNN_model.summary())

# Compile the model
RNN_model.compile(
    loss="binary_crossentropy",
    optimizer='adam',
    metrics=['accuracy']
)

# Train the model
print("\nTraining the model...")
history = RNN_model.fit(x_train, y_train,
                       batch_size=64,
                       epochs=5,
                       verbose=1,
                       validation_data=(x_valid, y_valid))

# Evaluate the model on test data
print("\nModel Evaluation on Test Data:")
test_score = RNN_model.evaluate(x_test, y_test, verbose=0)
print("Simple_RNN Test Accuracy: {:.2f}%".format(test_score[1]*100))

# Plot training history
plt.figure(figsize=(12, 5))

# Plot accuracy
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')

# Plot loss
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')

plt.tight_layout()
plt.show()





experiment 8

part one

from tensorflow.keras.preprocessing.text import one_hot

# Define the list of words
words = ['apple', 'banana', 'cherry', 'apple', 'cherry', 'banana', 'apple']

# Create a vocabulary of unique words
vocab = set(words)

# Assign a unique integer to each word in the vocabulary
word_to_int = {word: i for i, word in enumerate(vocab)}

# Convert the list of words to a list of integers using the vocabulary
int_words = [word_to_int[word] for word in words]

# Perform one-hot encoding of the integer sequence
one_hot_words = []
for int_word in int_words:
    one_hot_word = [0] * len(vocab)
    one_hot_word[int_word] = 1
    one_hot_words.append(one_hot_word)

print(one_hot_words)


part two

import string

# Define the input string
input_string = 'hello world'

# Create a vocabulary of unique characters
vocab = set(input_string)

# Assign a unique integer to each character in the vocabulary
char_to_int = {char: i for i, char in enumerate(vocab)}

# Convert the input string to a list of integers using the vocabulary
int_chars = [char_to_int[char] for char in input_string]

# Perform one-hot encoding of the integer sequence
one_hot_chars = []
for int_char in int_chars:
    one_hot_char = [0] * len(vocab)
    one_hot_char[int_char] = 1
    one_hot_chars.append(one_hot_char)

print(one_hot_chars)
